# Language Modeling - Text Generation with Markov Chain and LSTM

This work is about generating text by training language models based on Markov Chain and Recurrent Neural Networks (especially LSTM - Long Short Term Memory). Then we evaluate their effectiveness.

## Problem formulation

The goal of a language model is to probabilty of appearing a text.

- Chain rule:
<p align="center"><img src="/tex/1043de905170b00217d0c78afcbfed8f.svg?invert_in_darkmode&sanitize=true" align=middle width=292.49237489999996pt height=16.438356pt/></p>

- Markov assumption:
<p align="center"><img src="/tex/d19308cd1fea98fe6a7e1162d15d199e.svg?invert_in_darkmode&sanitize=true" align=middle width=421.86598245pt height=16.438356pt/></p>


## Markov Language Model

## LSTM Model
